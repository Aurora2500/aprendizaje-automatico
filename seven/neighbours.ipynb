{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica siete\n",
    "\n",
    "Grupo 14:\n",
    "* Joaquín Ibáñez Penalva\n",
    "* Aurora Zuoris\n",
    "\n",
    "Para la realización de esta práctica se usará la librería de numpy, pandas, matplotlib, y sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('precio_casas_clasificacion.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apartado 1\n",
    "\n",
    "X = data.drop('Precio', axis=1) # Eliminamos la columna precio\n",
    "y = data['Precio'] # Obtenemos la columna precio\n",
    "X.head() # Mostramos las 5 primeras filas de X\n",
    "y.head() # Mostramos las 5 primeras filas de y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Dividimos los datos en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apartado 2\n",
    "\n",
    "for i in range(1, 21): # Probamos con 20 vecinos\n",
    "    knn = KNeighborsClassifier(n_neighbors=i) # Creamos el modelo\n",
    "    knn.fit(X_train, y_train) # Entrenamos el modelo\n",
    "    y_pred = knn.predict(X_test) # Predecimos los valores\n",
    "    print(f'K={i:>2} -> Accuracy: {accuracy_score(y_test, y_pred):.5f}') # Mostramos el accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apartado 3\n",
    "\n",
    "scaler = MinMaxScaler() # Creamos el objeto scaler para escalar los datos entre 0 y 1\n",
    "X_train_scaled = scaler.fit_transform(X_train) # Escalamos los datos de entrenamiento\n",
    "X_test_scaled = scaler.transform(X_test) # Escalamos los datos de test\n",
    "\n",
    "Max_accuracy_apart3 = 0 # Variable para almacenar el accuracy máximo\n",
    "\n",
    "for i in range(1, 21): # Probamos con 20 vecinos\n",
    "    knn = KNeighborsClassifier(n_neighbors=i) # Creamos el modelo con el número de vecinos vecinos que queremos probar (i)\n",
    "    knn.fit(X_train_scaled, y_train) # Entrenamos el modelo\n",
    "    y_pred = knn.predict(X_test_scaled) # Predecimos los valores\n",
    "    print(f'K={i:>2} -> Accuracy scaled: {accuracy_score(y_test, y_pred):.5f}') # Mostramos el accuracy\n",
    "    if accuracy_score(y_test, y_pred) > Max_accuracy_apart3:\n",
    "        Max_accuracy_apart3 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede comprobar la tasa de acierto cuando está escalado es bastante mayor que cuando no lo está, ya que así nos aseguramos de que todo esté en la misma orden de magnitud por lo que todas las variables tienen la misma importancia independientemente de su escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apartado 4\n",
    "# Calcular tiempo de ejecución del apartado 3 para ball_tree, kd_tree y brute y printearlo\n",
    "\n",
    "inicio = time.time() # Guardamos el tiempo de inicio de la ejecución\n",
    "\n",
    "for j in range(1, 5): # Probamos con 4 iteraciones\n",
    "    for i in range(1, 21):\n",
    "        knn = KNeighborsClassifier(n_neighbors=i, algorithm='ball_tree') \n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "fin = time.time() # Guardamos el tiempo de fin de la ejecución\n",
    "print(f'Tiempo de ejecución ball tree: {fin - inicio:.5f}') # Mostramos el tiempo de ejecución\n",
    "\n",
    "inicio = time.time()\n",
    "\n",
    "for j in range(1, 5):\n",
    "    for i in range(1, 21):\n",
    "        knn = KNeighborsClassifier(n_neighbors=i, algorithm='kd_tree')\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "fin = time.time()\n",
    "print(f'Tiempo de ejecución kd tree: {fin - inicio:.5f}')\n",
    "\n",
    "inicio = time.time()\n",
    "\n",
    "for j in range(1, 5):\n",
    "    for i in range(1, 21):\n",
    "        knn = KNeighborsClassifier(n_neighbors=i, algorithm='brute')\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "fin = time.time()\n",
    "print(f'Tiempo de ejecución brute: {fin - inicio:.5f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ordenador en el que se elaboró este apartado, el que más tarda es el de ball tree con 50 segundos aprox y el que menos el de kd tree con 18 segundos aprox. El brute tarda 25 segundos aprox, menos que el ball tree pero más que el kd tree. Con lo que lo más óptimo es usar el kd tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apartado 5\n",
    "\n",
    "X_train_5, X_val, y_train_5, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42) # Dividimos los datos de entrenamiento en train y validación (80% train y 20% validación)\n",
    "\n",
    "print(\"------------ KNN con pesos uniformes --------------\")\n",
    "\n",
    "Max_accuracy_uniform = 0 # Variable para almacenar el accuracy máximo con pesos uniformes\n",
    "Max_accuracy_k_uniform = 0 # Variable para almacenar el k con el que se obtiene el accuracy máximo con pesos uniformes\n",
    "\n",
    "for i in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i, weights='uniform') # Probamos con pesos uniformes\n",
    "    knn.fit(X_train_5, y_train_5)\n",
    "    y_pred = knn.predict(X_val)\n",
    "    print(f'K={i:>2} -> Accuracy scaled weights = uniform: {accuracy_score(y_val, y_pred):.5f}')\n",
    "    if accuracy_score(y_val, y_pred) > Max_accuracy_uniform: # Si el accuracy es mayor que el máximo actual, lo guardamos\n",
    "        Max_accuracy_uniform = accuracy_score(y_val, y_pred) # Guardamos el accuracy máximo\n",
    "        Max_accuracy_k_uniform = i # Guardamos el k con el que se obtiene el accuracy máximo\n",
    "\n",
    "print(\"------------ KNN con pesos inversamente proporcionales a la distancia --------------\")\n",
    "\n",
    "Max_accuracy_distance = 0 # Variable para almacenar el accuracy máximo con pesos inversamente proporcionales a la distancia\n",
    "Max_accuracy_k_distance = 0 # Variable para almacenar el k con el que se obtiene el accuracy máximo con pesos inversamente proporcionales a la distancia\n",
    "\n",
    "for i in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i, weights='distance') # Probamos con pesos inversamente proporcionales a la distancia (más cercanos tienen más peso)\n",
    "    knn.fit(X_train_5, y_train_5)\n",
    "    y_pred = knn.predict(X_val)\n",
    "    print(f'K={i:>2} -> Accuracy scaled weights = distance: {accuracy_score(y_val, y_pred):.5f}')\n",
    "    if accuracy_score(y_val, y_pred) > Max_accuracy_distance:\n",
    "        Max_accuracy_distance = accuracy_score(y_val, y_pred)\n",
    "        Max_accuracy_k_distance = i\n",
    "\n",
    "print(f'El mejor accuracy con pesos uniformes es {Max_accuracy_uniform:.5f} con k = {Max_accuracy_k_uniform}')\n",
    "print(f'El mejor accuracy con pesos inversamente proporcionales a la distancia es {Max_accuracy_distance:.5f} con k = {Max_accuracy_k_distance}')\n",
    "\n",
    "if Max_accuracy_uniform > Max_accuracy_distance: # Comparamos los accuracy máximos para ver cuál es el mejor modelo\n",
    "    best_k = Max_accuracy_k_uniform\n",
    "    best_weights = 'uniform'\n",
    "    print(f'El mejor modelo es con pesos uniformes es y k = {best_k}')\n",
    "else:\n",
    "    best_k = Max_accuracy_k_distance\n",
    "    best_weights = 'distance'\n",
    "    print(f'El mejor modelo es con pesos inversamente proporcionales a la distancia y k = {best_k}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=best_k, weights=best_weights) # Creamos el modelo con el mejor k y los mejores pesos (uniform o distance, en este caso distance)\n",
    "knn.fit(X_train_scaled, y_train) # Entrenamos el modelo\n",
    "y_pred = knn.predict(X_test_scaled) # Predecimos los valores\n",
    "print(f'Accuracy final: {accuracy_score(y_test, y_pred):.5f}') # Mostramos el accuracy\n",
    "\n",
    "print(f'Max_accuracy_apart3: {Max_accuracy_apart3:.5f}') # Mostramos el accuracy máximo del apartado 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentroideMasProximo:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.centroides = None\n",
    "        self.labels = None\n",
    "\n",
    "    def ajustar(self, X, y):\n",
    "        self.labels = np.unique(y)\n",
    "        self.centroides = np.array([np.mean(X[y == label], axis=0) for label in self.labels])\n",
    "        return self\n",
    "\n",
    "    def predecir(self, X):\n",
    "        distancias = np.array([np.linalg.norm(X - centroide, axis=1) for centroide in self.centroides])\n",
    "        return self.labels[np.argmin(distancias, axis=0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.iloc[:, :-1].values, data.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  \n",
    "\n",
    "for train, test  in [(X_train, X_test), (X_train_scaled, X_test_scaled)]:\n",
    "  mine_tstart = time.time()\n",
    "  cmp = CentroideMasProximo().ajustar(train, y_train)\n",
    "  y_pred_mine = cmp.predecir(test)\n",
    "  mine_tend = time.time()\n",
    "  print(\"Nuestro:\", accuracy_score(y_pred_mine, y_test), f\"({mine_tend - mine_tstart:.2f}s)\")\n",
    "\n",
    "  their_tstart = time.time()\n",
    "  nc = NearestCentroid().fit(train, y_train)\n",
    "  y_pred_their = nc.predict(test)\n",
    "  their_tend = time.time()\n",
    "  print(\"Sklearn:\", accuracy_score(y_pred_their, y_test), f\"({their_tend - their_tstart:.2f}s)\")\n",
    "\n",
    "  cm = confusion_matrix(y_pred_their, y_pred_mine)\n",
    "\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nc.classes_)\n",
    "  disp.plot()\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que hay una concordancia del 100% entre la clase implementada y la de sklearn.\n",
    "En cuanto a la utilidad de este algoritmo para el problema, tienen una taza del acierto del 23% y una de 45% al escalar las variables, por lo que el escalado de variables ayuda, pero aún así nó es muy útil.\n",
    "\n",
    "A continuación está la matriz de confusión entre el predecido y el real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_mine)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nc.classes_).plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, se analiza el tiempo de ejecución de ambos algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "times_mine = []\n",
    "for _ in range(n):\n",
    "    tstart = time.time()\n",
    "    cmp = CentroideMasProximo().ajustar(X_train_scaled, y_train)\n",
    "    y_pred_mine = cmp.predecir(X_test_scaled)\n",
    "    tend = time.time()\n",
    "    times_mine.append(tend - tstart)\n",
    "\n",
    "times_their = []\n",
    "\n",
    "for _ in range(n):\n",
    "    tstart = time.time()\n",
    "    nc = NearestCentroid().fit(X_train_scaled, y_train)\n",
    "    y_pred_their = nc.predict(X_test_scaled)\n",
    "    tend = time.time()\n",
    "    times_their.append(tend - tstart)\n",
    "\n",
    "mine_mean = np.mean(times_mine)\n",
    "mine_std = np.std(times_mine)\n",
    "\n",
    "their_mean = np.mean(times_their)\n",
    "their_std = np.std(times_their)\n",
    "\n",
    "print(f\"Nuestro \\t μ: {mine_mean:.4f}s \\t σ: {mine_std:.4f}s\")\n",
    "print(f\"Sklearn \\t μ: {their_mean:.4f}s \\t σ: {their_std:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (mine_mean - their_mean) / np.sqrt(mine_std**2 / n + their_std**2 / n)\n",
    "print(f\"z: {z:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto al tiempo de ejecución de ambos, se puede ver que el de sklearn es más rapido, ejecutandose por entre 10% y 20% más rapido. Y además tiene una menor desviación típica en su ejecución.\n",
    "Calculando el z-score de la diferencia de ejecución, esta sale entre 10 y 20.\n",
    "Teniendo en cuenta que un p-valor del 5% corresponde a un z-score de 1.96, se puede decir que la diferencia de ejecución es bastante significativa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "124093b7be10c32076c75f60f415d6def65edeb693f6c465ae4e1e508e9d8137"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
