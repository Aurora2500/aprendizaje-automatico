{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica diez\n",
    "\n",
    "Grupo 14:\n",
    "* Joaquín Ibáñez Penalva\n",
    "* Aurora Zuoris\n",
    "\n",
    "Para la realización de esta práctica se usará la librería de numpy, pandas, matplotlib, y sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('CelebA-10K-train.csv')\n",
    "data_test = pd.read_csv('CelebA-10K-test.csv')\n",
    "\n",
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.iloc[:, 2:]\n",
    "y_train = data_train.iloc[:, 1]\n",
    "X_test = data_test.iloc[:, 2:]\n",
    "y_test = data_test.iloc[:, 1]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "classifs = [\n",
    "\t('Regresión Logística', LogisticRegression(random_state=42)),\n",
    "\t('Perceptón', Perceptron(random_state=42)),\n",
    "\t('SVM', SVC(random_state=42)),\n",
    "\t('KNN(5)', KNeighborsClassifier(n_neighbors=5)),\n",
    "\t('Centroide más cercano', NearestCentroid())\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "table = defaultdict(list)\n",
    "\n",
    "fig, ax = plt.subplots(5, 1, figsize=(5, 25))\n",
    "\n",
    "for i, (name, model) in enumerate(classifs):\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\tcm = confusion_matrix(y_test, model.predict(X_test))\n",
    "\tConfusionMatrixDisplay(cm, display_labels=['male', 'female']).plot(ax=ax[i])\n",
    "\tax[i].set_title(name)\n",
    "\n",
    "\taccuracy = np.trace(cm) / np.sum(cm)\n",
    "\trecall = cm[1, 1] / np.sum(cm[1, :])\n",
    "\tprecision = cm[1, 1] / np.sum(cm[:, 1])\n",
    "\tspecificity = cm[0, 0] / np.sum(cm[0, :])\n",
    "\tf1_score = 2 * precision * recall / (precision + recall)\n",
    "\ttable['model'].append(name)\n",
    "\ttable['accuracy'].append(accuracy)\n",
    "\ttable['recall'].append(recall)\n",
    "\ttable['precision'].append(precision)\n",
    "\ttable['specificity'].append(specificity)\n",
    "\ttable['f1_score'].append(f1_score)\n",
    "\n",
    "pd.DataFrame(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El clasificador SVM es el que mayor accuracy ofrece con un 94%, seguido de la regresión logística que nos ofrece un 91%. Respecto a la sensibilidad, el SVM vuelve a ser el mejor con mucha diferencia con un 95%. En cuanto a la precisión, gana el knn con 5 vecinos con casi un 95%, seguido del 92% del SVM. La especificidad la vuelve a ganar el knn con un 97%, otra vez seguido del 94% del SVM. El mayor f1 se lo lleva con mucha diferencia el SVM con un 93%, el siguiente más cercano es la regresión logística un un 89%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.iloc[:,2:]\n",
    "y_train = data_train.iloc[:,1]\n",
    "X_test = data_test.iloc[:,2:]\n",
    "y_test = data_test.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import attrs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression(random_state=42)),\n",
    "    (\"Perceptron\", Perceptron(random_state=42)),\n",
    "    (\"SVC\", SVC(random_state=42, gamma=\"auto\")),\n",
    "    (\"Gaussian\", GaussianNB()),\n",
    "    (\"Nearest Centroid\", NearestCentroid()),\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "fig2, ax2 = plt.subplots(5, 1, figsize=(5, 25))\n",
    "\n",
    "for i, (name, model) in enumerate(models):\n",
    "    model.fit(X_train, y_train)\n",
    "    p = model.predict(X_test)\n",
    "    c = confusion_matrix(y_test, p)\n",
    "    ConfusionMatrixDisplay(c, display_labels=[\"male\", \"female\"]).plot(ax=ax2[i])\n",
    "    ax2[i].set_title(name)\n",
    "\n",
    "    tn, fp, fn, tp = c.ravel()\n",
    "    \n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "\n",
    "    ax.scatter(fpr, tpr, label=name)\n",
    "    ax.plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")\n",
    "\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\")\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar con la regresión logística es la que mayor ratio de verdaderos positivos tiene, así como la que menor ratio de falsos positivos tiene. Por lo tanto, es la que mejor clasifica los datos. Por otro lado, la Gausiana es la que peor clasifica los datos, ya que tiene el menor ratio de verdaderos positivos y el segundo mayor ratio de falsos positivos, solo por detrás del Nearest Centroid, sin embargo el Nearest Centroid al menos tiene una tasa de verdaderos positivos prácticamente igual que el de la regresión logística."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linspace\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "classifs = [    ('Naive Bayes', GaussianNB()),    ('Regresión Logística', LogisticRegression()),    ('SVM', SVC(probability=True))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "for name, model in classifs:\n",
    "    model.fit(X_train, y_train)\n",
    "    probs = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, label=f\"{name} (AUC = {auc_score:.2f})\")\n",
    "\n",
    "# Añadir la línea punteada que representa el clasificador aleatorio\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Clasificador aleatorio')\n",
    "\n",
    "# Ajustar los límites de los ejes x e y\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Agregar etiquetas y leyenda\n",
    "ax.set_xlabel('Tasa de Falsos Positivos')\n",
    "ax.set_ylabel('Tasa de Verdaderos Positivos')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "124093b7be10c32076c75f60f415d6def65edeb693f6c465ae4e1e508e9d8137"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
