{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica tres\n",
    "\n",
    "Grupo 14:\n",
    "* Joaquín Ibáñez Penalva\n",
    "* Aurora Zuoris\n",
    "\n",
    "Para la realización de esta práctica  se usará la librería de numpy, pandas, matplotlib y sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('precio_casas.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_numpy()\n",
    "x = data[:, :-1] # todas las filas, todas las columnas menos la última\n",
    "y = data[:, -1] # todas las filas, la última columna\n",
    "fig, ax = plt.subplots(2, 4, figsize=(32, 16)) # 2 filas, 4 columnas\n",
    "idx_to_r2 = [] # índice de la columna y su R2\n",
    "\n",
    "for i, (header, vec) in enumerate(zip(df.columns, x.T)): # recorremos las columnas\n",
    "    col = vec.reshape(-1, 1) # convertimos a matriz\n",
    "    reg = lm.LinearRegression().fit(col, y) # ajustamos el modelo\n",
    "    r2 = r2_score(y, reg.predict(col)) # calculamos el R2\n",
    "    idx_to_r2.append((i, r2)) # guardamos el índice y el R2\n",
    "    ax[i // 4, i % 4].scatter(vec, y, color='blue') # dibujamos los puntos\n",
    "    ax[i // 4, i % 4].plot(col, col * reg.coef_[0] + reg.intercept_, color='red') # dibujamos la recta de regresión lineal\n",
    "    cor = np.corrcoef(vec, y)[0, 1] # calculamos la correlación\n",
    "    ax[i // 4, i % 4].set_title(f\"{header}: r2={r2:.5f} cor={cor:.4f}\") # título con el R2 redondeado a 5 decimales\n",
    "    ax[i // 4, i % 4].set_ylim(0, 1.05 * y.max())\n",
    "    ax[i // 4, i % 4].set_xlabel(header)\n",
    "    ax[i // 4, i % 4].set_ylabel('Precio')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un valor de $R^2$ cercano a 1 indica una fuerte dependencia lineal entre la variable independiente y la variable dependiente, mientras que un valor cercano a 0 indica una dependencia lineal débil o nula.\n",
    "En este caso, se puede apreciar claramente como hay una correlación entre la mediana de ingresos y la del precio mayor que el resto de variables en las que la dependencia es muy débil."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "Primero se dividen los datos en un conjunto de entrenamiento y otro de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_numpy()\n",
    "\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print(X_train.shape, y_train.shape, )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener las columnas ordenadas segun el $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_cols = [x for x, _ in sorted(idx_to_r2, key=lambda x: x[1], reverse=True)]\n",
    "print(ordered_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_collection = []\n",
    "\n",
    "for i in range(1, 9):\n",
    "    lim_cols = ordered_cols[:i]\n",
    "    model = lm.LinearRegression()\n",
    "    model.fit(X_train[:, lim_cols], y_train)\n",
    "    y_pred = model.predict(X_test[:, lim_cols])\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    rmse_collection.append(rmse)\n",
    "\n",
    "data = {\n",
    "    'Features': df.columns[ordered_cols],\n",
    "    'RMSE': rmse_collection,\n",
    "}\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que el error baja con el número de variables. Aunque se puede apreciar que sube un poco con la adición de la segunda variable comparado a si solo usase una única variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c900cb90dee983911779de55d7a1efccb46f16c81cdf15a7b8ee95d3de3d3b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
